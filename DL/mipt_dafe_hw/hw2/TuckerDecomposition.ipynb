{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjl6x6og3uXH"
      },
      "source": [
        "# HW 2 - Разложение матриц градиентным методом"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv79QFb_-oNZ"
      },
      "source": [
        "Цель задания: В ходе реализации [разложения Таккера](https://proceedings.neurips.cc/paper/2018/file/45a766fa266ea2ebeb6680fa139d2a3d-Paper.pdf) градиентным методом освоить pyTorch и реализовать подходы оптимизации параметров модели (в отсутствии готовых решений)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7cETheA1BbT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1cdc490-2ce9-4e4c-e78d-ea34fcd0eee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorly\n",
            "  Downloading tensorly-0.9.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorly) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from tensorly) (1.13.1)\n",
            "Downloading tensorly-0.9.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorly\n",
            "Successfully installed tensorly-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HUSrylpBwYn"
      },
      "source": [
        "[Более-менее внятное описание алгоритма канонического разложения](https://www.alexejgossmann.com/tensor_decomposition_tucker/) - само аналитическое разложение вам реализовывать НЕ НУЖНО"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1PuoBtG7iw7",
        "outputId": "413faafc-dcae-41e6-abba-08f126feec5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x79d43b058170>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorly as tl\n",
        "from tensorly.decomposition import tucker\n",
        "from tensorly.metrics.regression import RMSE\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tqdm.notebook import tqdm\n",
        "from torch import nn\n",
        "\n",
        "import math\n",
        "from torch.optim.optimizer import Optimizer\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdgOrKhyCS6r",
        "outputId": "adf2535e-8054-4663-edc1-bc3dad0410ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LfhKpuX7htE"
      },
      "source": [
        "## 1 Создайте 3х мерный тензор\n",
        "Размер тензора не меньше 100 по каждой из размерностей.\n",
        "\n",
        "Заполните случайными целыми числами в диапазоне от 0 до 9."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap1Ozn7P8-Yj"
      },
      "source": [
        "Примечание: разложение будет корректно работать со случайным тензором, только если изначально создавать случайные ядро и матрицы, а потом по ним формировать тензор. Работайте с типом *torch.Tensor.double*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IXgWKromr6Vi"
      },
      "outputs": [],
      "source": [
        "# # Функция, восстанавливающая тензор по ядру и матрицам\n",
        "# def repair_tensor(G_, U):\n",
        "#     # data - восстановленный тензор из матриц и ядра\n",
        "#     # U - список матриц\n",
        "#     # G_ - ядро разложения\n",
        "#     a1 = tl.tenalg.mode_dot(tensor=tl.tensor(G_.detach().numpy()), matrix_or_vector=tl.tensor(U[0].detach().numpy()), mode=0, transpose=False)\n",
        "#     a2 = tl.tenalg.mode_dot(tensor=a1, matrix_or_vector=tl.tensor(U[1].detach().numpy()), mode=1, transpose=False)\n",
        "#     a3 = tl.tenalg.mode_dot(tensor=a2, matrix_or_vector=tl.tensor(U[2].detach().numpy()), mode=2, transpose=False)\n",
        "#     return torch.tensor(a3, dtype=torch.double, requires_grad=True, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5SzHzteOROQQ"
      },
      "outputs": [],
      "source": [
        "# Создадим тензор: размер тензора и r задаётся\n",
        "def get_tensor(size=(100,200,150), r=10):\n",
        "    # data - тензор с заданной размерностью\n",
        "    # U - список матриц\n",
        "    U = [torch.randn(size[i], r, dtype=torch.double) for i in range(len(size))]\n",
        "    # G - ядро разложения\n",
        "    G = torch.randint(0, 10, (r, r, r), dtype=torch.double)\n",
        "    data = rebuild_tensor(G, U)\n",
        "\n",
        "    return data, U, G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Q4Vqcg71SaaE"
      },
      "outputs": [],
      "source": [
        "def rebuild_tensor(G_, U):\n",
        "    result = G_\n",
        "    for i, u in enumerate(U):\n",
        "        result = torch.tensordot(result, u, dims=([0], [1]))\n",
        "    # print(f\"result tensor requires_grad: {result.requires_grad}\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFuFlp2n78Tz"
      },
      "source": [
        "Сгенерируйте тензор и добавьте к нему случайный шум с размерностью *1e-2*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnUbbsYSdrsw",
        "outputId": "bc629141-4d9f-4658-f7f2-b84170b2ec59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([100, 200, 300]),\n",
              " [torch.Size([100, 10]), torch.Size([200, 10]), torch.Size([300, 10])],\n",
              " torch.Size([10, 10, 10]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "size = (100, 200, 300)\n",
        "r = 10\n",
        "\n",
        "data, U, G = get_tensor(size, r)\n",
        "data.shape, [u.shape for u in U], G.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZz0iCIYH5oJ",
        "outputId": "3446adbd-15bd-4775-bdad-79f1549647ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_w_noise requires_grad: False\n"
          ]
        }
      ],
      "source": [
        "data_w_noise = data + torch.randn_like(data) * 1e-2\n",
        "\n",
        "print(f\"data_w_noise requires_grad: {data_w_noise.requires_grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp75_Ad29RL5"
      },
      "source": [
        "Вопрос:\n",
        "Почему задание не имеет смысла для полностью случайного тензора и зачем добавлять шум? *не отвечать нельзя*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VLMaT5wyE11"
      },
      "source": [
        "Ответ:\n",
        "> Тензорное разложение, например Tucker decomposition, предполагает наличие скрытой структуры в данных — зависимостей между компонентами, которые можно выразить через ядро и факторные матрицы.\n",
        "> У случайного тензора скрытая структура отсутствует. Все элементы независимы, и их невозможно корректно представить с помощью матриц или ядра.\n",
        "\n",
        "> Добавление шума используется для проверки устойчивости алгоритмов восстановления тензора в условиях, приближенных к реальным данным."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzninpMYD_hd"
      },
      "source": [
        "## 2 Реализуйте метод для восстановления тензора по разложению"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "YDTx9ZbYD-_S"
      },
      "outputs": [],
      "source": [
        "# Функция, восстанавливающая тензор по ядру и матрицам (для результата из библы tensorly)\n",
        "def repair_tensor_for_ndarray(G_, U):\n",
        "    # data - восстановленный тензор из матриц и ядра\n",
        "    # U - список матриц\n",
        "    # G_ - ядро разложения\n",
        "    a1 = tl.tenalg.mode_dot(tensor=tl.tensor(G_), matrix_or_vector=tl.tensor(U[0]), mode=0, transpose=False)\n",
        "    a2 = tl.tenalg.mode_dot(tensor=a1, matrix_or_vector=tl.tensor(U[1]), mode=1, transpose=False)\n",
        "    a3 = tl.tenalg.mode_dot(tensor=a2, matrix_or_vector=tl.tensor(U[2]), mode=2, transpose=False)\n",
        "    return torch.tensor(a3, dtype=torch.double)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKqzxtaE-F16"
      },
      "source": [
        "## 3 Сделайте разложение библиотечным методом\n",
        "Пакет можете брать любой"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hlp4Jh3--fKh",
        "outputId": "99b7a98d-0cfd-45c3-b10f-cadcf4b318fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 10, 10) [(100, 10), (200, 10), (300, 10)]\n"
          ]
        }
      ],
      "source": [
        "from tensorly.decomposition import tucker\n",
        "from tensorly import tucker_to_tensor\n",
        "\n",
        "# использую tucker from tensorly\n",
        "# data_ndarray = data_w_noise.detach().cpu().numpy()\n",
        "data_ndarray = data_w_noise.detach().numpy()\n",
        "core, factors = tucker(tl.tensor(data_ndarray), [r, r, r])\n",
        "print(core.shape, [u.shape for u in factors])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMw1x8w8-lsh"
      },
      "source": [
        "Не забудьте померить ошибку разложения по метрике MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaBCzVUcY5BH"
      },
      "outputs": [],
      "source": [
        "def MSE(tensor1, tensor2):\n",
        "    delta = tensor1-tensor2\n",
        "    delta *= delta\n",
        "    mse = delta.sum() / delta.numel()\n",
        "    return mse.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWkdb7Ip-mL3"
      },
      "outputs": [],
      "source": [
        "repaired_data = repair_tensor_for_ndarray(core, factors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXz8pgtLXo09",
        "outputId": "2818264f-bd53-48d0-cef3-80074fef2f64"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9.993324875311374e-05"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MSE(repaired_data, data_w_noise)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibOgeEgfD1wm"
      },
      "source": [
        "## 4 Реализуйте разложение градиентным методом"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GstBYmiBF7A6"
      },
      "source": [
        "### 4.1 Реализуйте *optimizer*\n",
        "Можно взять из исходников *PyTorch* и отнаследоваться от *torch.optim.optimizer*.\n",
        "Используйте квадратичный *Loss*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pz_Gt7mN1Tu8"
      },
      "outputs": [],
      "source": [
        "class TuckerOptimizer(Optimizer):\n",
        "    def __init__(self, params, lr=1e-3):\n",
        "        defaults = {'lr': lr}\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    # по мотивам доков sgd optimizer\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            for param in group['params']:\n",
        "                if param.grad is not None:\n",
        "                    param.data -= group['lr'] * param.grad.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GSolH5dEJba"
      },
      "source": [
        "### 4.2 Реализуйте цикл оптимизации параметров"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6UWpuERFTn8"
      },
      "source": [
        "Стоит параметры оптимизировать сразу на GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7VNTESw-M7Rd"
      },
      "outputs": [],
      "source": [
        "def train(data, rank, lr=1e-3, epochs=1000):\n",
        "    history = []\n",
        "\n",
        "    factors = [torch.randn(data.size(i), rank[i], requires_grad=True, dtype=torch.double) for i in range(3)]\n",
        "    core_tensor = torch.randn(rank, requires_grad=True, dtype=torch.double)\n",
        "\n",
        "    optimizer = TuckerOptimizer([core_tensor] + factors, lr=lr)\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        approx_tensor = rebuild_tensor(core_tensor, factors)\n",
        "        loss = loss_fn(approx_tensor, data)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if (epoch+1) % 100 == 0:\n",
        "            history.append(loss.item())\n",
        "            print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
        "\n",
        "    return core_tensor, factors, history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------> Чтобы до ошибки порядка 1e-4 добить надо 8-9к эпох <-------"
      ],
      "metadata": {
        "id": "yKDFdsTVzvQu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BGahONXxNrTH",
        "outputId": "642a995e-290f-4c97-93fb-fdd3a197bc48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100, Loss: 32226.686422491446\n",
            "Epoch 200, Loss: 32059.961120637898\n",
            "Epoch 300, Loss: 31983.610224391923\n",
            "Epoch 400, Loss: 31940.239032368245\n",
            "Epoch 500, Loss: 31911.584485169922\n",
            "Epoch 600, Loss: 31889.28796479664\n",
            "Epoch 700, Loss: 31866.572879305764\n",
            "Epoch 800, Loss: 31828.20210813216\n",
            "Epoch 900, Loss: 31669.432192945962\n",
            "Epoch 1000, Loss: 20614.415556480806\n",
            "Epoch 1100, Loss: 7969.444189170975\n",
            "Epoch 1200, Loss: 7950.139415317571\n",
            "Epoch 1300, Loss: 7938.905294285679\n",
            "Epoch 1400, Loss: 7930.7578211269965\n",
            "Epoch 1500, Loss: 7923.776889862335\n",
            "Epoch 1600, Loss: 7916.961178118073\n",
            "Epoch 1700, Loss: 7909.594338379353\n",
            "Epoch 1800, Loss: 7900.985878501701\n",
            "Epoch 1900, Loss: 7890.308565666563\n",
            "Epoch 2000, Loss: 7876.436236736562\n",
            "Epoch 2100, Loss: 7857.7279748385545\n",
            "Epoch 2200, Loss: 7831.710814922136\n",
            "Epoch 2300, Loss: 7794.623671593929\n",
            "Epoch 2400, Loss: 7740.867202639407\n",
            "Epoch 2500, Loss: 7662.73952579543\n",
            "Epoch 2600, Loss: 7551.731849650686\n",
            "Epoch 2700, Loss: 7403.5153865110615\n",
            "Epoch 2800, Loss: 7224.839328678118\n",
            "Epoch 2900, Loss: 7030.959126198919\n",
            "Epoch 3000, Loss: 6833.542144893352\n",
            "Epoch 3100, Loss: 6637.614209245321\n",
            "Epoch 3200, Loss: 6443.254117606303\n",
            "Epoch 3300, Loss: 6243.861552397244\n",
            "Epoch 3400, Loss: 6027.295708029126\n",
            "Epoch 3500, Loss: 5781.098085256975\n",
            "Epoch 3600, Loss: 5499.919700775966\n",
            "Epoch 3700, Loss: 5188.606436799376\n",
            "Epoch 3800, Loss: 4850.723551570629\n",
            "Epoch 3900, Loss: 4478.254901633484\n",
            "Epoch 4000, Loss: 4067.1808106829953\n",
            "Epoch 4100, Loss: 3641.0617640576374\n",
            "Epoch 4200, Loss: 3241.481460333923\n",
            "Epoch 4300, Loss: 2890.3893965080024\n",
            "Epoch 4400, Loss: 2581.9019491157255\n",
            "Epoch 4500, Loss: 2300.8721974321493\n",
            "Epoch 4600, Loss: 2035.4706531945299\n",
            "Epoch 4700, Loss: 1782.612923333931\n",
            "Epoch 4800, Loss: 1548.7997831685943\n",
            "Epoch 4900, Loss: 1341.9723530688493\n",
            "Epoch 5000, Loss: 1158.969628083607\n",
            "MSE после градиентного разложения: 1.157219e+03\n",
            "CPU times: user 4min 55s, sys: 6min 8s, total: 11min 3s\n",
            "Wall time: 11min 6s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x79d34edaa020>]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAGdCAYAAADwjmIIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/pElEQVR4nO3de3yU9Z33//dMkpkkkJkkQBICIZwqGI4VJGY9dNWUqNSV6t6LlttSRF1tcEVcRe5txe4JF3/taivqdruV7v2rB9hH0RYUS8OpSgAJRk6SKgYThEk4ZSYJySSZ+d5/hBmZcpqQCTMTXs/H43owc12fmfnO1azz3uv6XtfHYowxAgAA6GWs0R4AAABATyDkAACAXomQAwAAeiVCDgAA6JUIOQAAoFci5AAAgF6JkAMAAHolQg4AAOiVEqM9gGjy+/06dOiQ0tLSZLFYoj0cAAAQBmOMGhsblZubK6v13MdrLuuQc+jQIeXl5UV7GAAA4CLU1tZq8ODB59x+WYectLQ0SZ07yeFwRHk0AAAgHB6PR3l5ecHf8XO5rENO4BSVw+Eg5AAAEGcuNNWEiccAAKBXIuQAAIBeiZADAAB6JUIOAADolQg5AACgVyLkAACAXomQAwAAeiVCDgAA6JUIOQAAoFci5AAAgF6JkAMAAHolQg4AAOiVLusGnT3luff2ySKLxg5yaEyuU4MzUi7YRAwAAEQWISfCjDH69dYaNZxsD65LT03S2FynxgxyaGyuU2MHOZWfmSqrleADAEBPIeREWIff6PFvXqHdX3q0+5Bbf6prVMPJdr3/2VG9/9nRYF2aPVGjctKU7UhW/7429e9rV/80e+e/ged97UqxJUTx2wAAEL8IORGWlGDVvUVDg8+9HT59WtekXV+6tftLt3Yf8uiTwx41eju0/YsTF3y/PrYEpSUnqY89QX3tiepzaul8nND52JaoFFuC7EkJsidalZyUoOREq+yn/5tklT0xQUkJFtkSrbInJMiWaFVSgkWJCUzNAgD0PoScHmZPTNDYQZ2nqALafX7tP9KkP9U16WijV8eavTra2KajTd5TS5uONHnV1uFXc5tPzW2+Hh2j1SLZEq2yJVhD/z21JCV8tc6e2BmWUmwJSrUlKNWWqD62zud97IlfrbMnaMSAvspKszMfCQAQFYScKEhKsGp0jkOjcxznrDHGqNHboeNNbWrydqixtUPN3g41t3Woydv5uMnr61zn7VBLu0/edr9aO87xb7tPbR1+eX1+tXX4Qz7Lb6TWdr9a2/3nGM3F69fHpoJchwoGOlSQ69CVAx0a3r8PR48AAD2OkBOjLBaLHMlJciQnRfy9jTHq8Bu1dfjVfir0eDv8ajv1OLDu9FD057Un23w62dYR8m+z16eW9g41e33ytLTrwLFmHWtu0x8/Pao/fvrVfCR7olWjctJUMNCh/31NfshRLgAAIoWQcxmyWCxKSrAoqYePprS0+fSnukbtPezR3kMe7T3s0b7DHjW3+bTzoFs7D7pVVdeold+/tkfHAQC4PBFy0GNSbAmakJeuCXnpwXV+v1HN8ZP6wyd1+ufVn6jO3Rq9AQIAejUmRuCSslotGtq/j6YW5EiSjp9si/KIAAC9FSEHUZHRp3OuUWu7Xy09fPUYAODyRMhBVPS1JyopofPS8hMczQEA9ABCDqLCYrEoPdUmSTreTMgBAEQeIQdRk3kq5Jze5wsAgEgh5CBqAvNymHwMAOgJhBxETWafziM5JzhdBQDoAYQcRE1gTg4TjwEAPYGQg6gJzMnhSA4AoCcQchA1GadOVx1n4jEAoAcQchA1GamdE48bOF0FAOgBhBxETfBIDqerAAA9oEsh5+WXX9b48ePlcDjkcDhUVFSkd999N7i9tbVVpaWl6tevn/r27au77rpLdXV1Ie9RU1OjadOmKTU1VVlZWXriiSfU0dERUrNhwwZdddVVstvtGjlypJYtW3bGWJYuXaqhQ4cqOTlZhYWF2rZtW1e+CmIAc3IAAD2pSyFn8ODBevbZZ1VRUaHt27frpptu0h133KE9e/ZIkh577DH97ne/04oVK7Rx40YdOnRId955Z/D1Pp9P06ZNU1tbmzZv3qxf/epXWrZsmZ5++ulgTXV1taZNm6Ybb7xRlZWVmjdvnu6//3699957wZo333xT8+fP16JFi7Rjxw5NmDBBJSUlqq+v7+7+wCWUEbjjMaerAAA9wXRTRkaG+cUvfmEaGhpMUlKSWbFiRXDbJ598YiSZ8vJyY4wx77zzjrFarcblcgVrXn75ZeNwOIzX6zXGGPPkk0+aMWPGhHzGjBkzTElJSfD5lClTTGlpafC5z+czubm5ZvHixV0au9vtNpKM2+3u0usQGZ6WNpO/YJXJX7DKnPR2RHs4AIA4Ee7v90XPyfH5fHrjjTfU3NysoqIiVVRUqL29XcXFxcGa0aNHa8iQISovL5cklZeXa9y4ccrOzg7WlJSUyOPxBI8GlZeXh7xHoCbwHm1tbaqoqAipsVqtKi4uDtYgPtCkEwDQkxK7+oJdu3apqKhIra2t6tu3r1auXKmCggJVVlbKZrMpPT09pD47O1sul0uS5HK5QgJOYHtg2/lqPB6PWlpadOLECfl8vrPW7Nu377xj93q98nq9wecejyf8L46ICzTpPNLo1fHmNuWmp0R7SACAXqTLR3JGjRqlyspKbd26VQ8//LBmzZqlvXv39sTYIm7x4sVyOp3BJS8vL9pDuuzRpBMA0FO6HHJsNptGjhypSZMmafHixZowYYJeeOEF5eTkqK2tTQ0NDSH1dXV1ysnJkSTl5OSccbVV4PmFahwOh1JSUtS/f38lJCSctSbwHueycOFCud3u4FJbW9vVr48Io0knAKCndPs+OX6/X16vV5MmTVJSUpLKysqC26qqqlRTU6OioiJJUlFRkXbt2hVyFdTatWvlcDhUUFAQrDn9PQI1gfew2WyaNGlSSI3f71dZWVmw5lzsdnvw8vfAguiiSScAoKd0aU7OwoULdeutt2rIkCFqbGzUa6+9pg0bNui9996T0+nUnDlzNH/+fGVmZsrhcOiRRx5RUVGRrrnmGknS1KlTVVBQoHvvvVdLliyRy+XSD37wA5WWlsput0uSHnroIb344ot68skndd9992ndunVavny5Vq9eHRzH/PnzNWvWLE2ePFlTpkzR888/r+bmZs2ePTuCuwaXAk06AQA9pUshp76+Xt/97nd1+PBhOZ1OjR8/Xu+9956++c1vSpL+/d//XVarVXfddZe8Xq9KSkr00ksvBV+fkJCgVatW6eGHH1ZRUZH69OmjWbNm6R//8R+DNcOGDdPq1av12GOP6YUXXtDgwYP1i1/8QiUlJcGaGTNm6MiRI3r66aflcrk0ceJErVmz5ozJyIh93BAQANBTLMYYE+1BRIvH45HT6ZTb7ebUVZT81/vV+qdVe3X7hFz97J6vR3s4AIA4EO7vN72rEFWBJp0cyQEARBohB1EVaNLJnBwAQKQRchBVzMkBAPQUQg6iiiadAICeQshBVAVuBtja7ldLmy/KowEA9CaEHEQVTToBAD2FkIOoslgsX52yYl4OACCCCDmIugyadAIAegAhB1FHk04AQE8g5CDqaNIJAOgJhBxEHU06AQA9gZCDqOOGgACAnkDIQdQFWjscZ+IxACCCCDmIOpp0AgB6AiEHUUeTTgBATyDkIOqYkwMA6AmEHEQdTToBAD2BkIOoo0knAKAnEHIQdTTpBAD0BEIOoo4mnQCAnkDIQUygSScAINIIOYgJNOkEAEQaIQcxgSadAIBII+QgJqQzJwcAEGGEHMSEzOCcHEIOACAyCDmICTTpBABEGiEHMYEmnQCASCPkICbQpBMAEGmEHMQEmnQCACKNkIOYkNmHJp0AgMgi5CAmpKfSpBMAEFmEHMQEmnQCACKNkIOYQJNOAECkEXIQM2jSCQCIJEIOYgZNOgEAkUTIQcygSScAIJIIOYgZNOkEAEQSIQcxgyadAIBIIuQgZtCkEwAQSYQcxAyadAIAIomQg5hBk04AQCQRchAzaNIJAIgkQg5iBk06AQCR1KWQs3jxYl199dVKS0tTVlaWpk+frqqqqpCav/zLv5TFYglZHnrooZCampoaTZs2TampqcrKytITTzyhjo6OkJoNGzboqquukt1u18iRI7Vs2bIzxrN06VINHTpUycnJKiws1LZt27rydRBjaNIJAIikLoWcjRs3qrS0VFu2bNHatWvV3t6uqVOnqrm5OaTugQce0OHDh4PLkiVLgtt8Pp+mTZumtrY2bd68Wb/61a+0bNkyPf3008Ga6upqTZs2TTfeeKMqKys1b9483X///XrvvfeCNW+++abmz5+vRYsWaceOHZowYYJKSkpUX19/sfsCUUaTTgBAJFmMMeZiX3zkyBFlZWVp48aNuuGGGyR1HsmZOHGinn/++bO+5t1339W3vvUtHTp0SNnZ2ZKkV155RQsWLNCRI0dks9m0YMECrV69Wrt37w6+7u6771ZDQ4PWrFkjSSosLNTVV1+tF198UZLk9/uVl5enRx55RE899VRY4/d4PHI6nXK73XI4HBe7GxBBU/7lD6pv9GrVI9dp7CBntIcDAIhB4f5+d2tOjtvtliRlZmaGrP/1r3+t/v37a+zYsVq4cKFOnjwZ3FZeXq5x48YFA44klZSUyOPxaM+ePcGa4uLikPcsKSlReXm5JKmtrU0VFRUhNVarVcXFxcEaxKdAk06O5AAAuivxYl/o9/s1b948XXvttRo7dmxw/Xe+8x3l5+crNzdXO3fu1IIFC1RVVaXf/OY3kiSXyxUScCQFn7tcrvPWeDwetbS06MSJE/L5fGet2bdv3znH7PV65fV6g889Hs9FfHP0pECTzhPcEBAA0E0XHXJKS0u1e/duvf/++yHrH3zwweDjcePGaeDAgbr55pu1f/9+jRgx4uJHGgGLFy/Wj370o6iOAedHk04AQKRc1OmquXPnatWqVVq/fr0GDx583trCwkJJ0meffSZJysnJUV1dXUhN4HlOTs55axwOh1JSUtS/f38lJCSctSbwHmezcOFCud3u4FJbWxvGt8WlRJNOAECkdCnkGGM0d+5crVy5UuvWrdOwYcMu+JrKykpJ0sCBAyVJRUVF2rVrV8hVUGvXrpXD4VBBQUGwpqysLOR91q5dq6KiIkmSzWbTpEmTQmr8fr/KysqCNWdjt9vlcDhCFsQWmnQCACKlS6erSktL9dprr+ntt99WWlpacA6N0+lUSkqK9u/fr9dee0233Xab+vXrp507d+qxxx7TDTfcoPHjx0uSpk6dqoKCAt17771asmSJXC6XfvCDH6i0tFR2u12S9NBDD+nFF1/Uk08+qfvuu0/r1q3T8uXLtXr16uBY5s+fr1mzZmny5MmaMmWKnn/+eTU3N2v27NmR2jeIApp0AgAixnSBpLMur776qjHGmJqaGnPDDTeYzMxMY7fbzciRI80TTzxh3G53yPscOHDA3HrrrSYlJcX079/fPP7446a9vT2kZv369WbixInGZrOZ4cOHBz/jdD/72c/MkCFDjM1mM1OmTDFbtmzpytcxbrfbSDpjfIie3+yoNfkLVpmZ/9m1/y0BAJePcH+/u3WfnHjHfXJiz/qqes1+9UONyXVo9d9dH+3hAABi0CW5Tw4QaTTpBABECiEHMYUmnQCASCHkIKbQpBMAECmEHMQUmnQCACKFkIOYYrFYgv2ruCEgAKA7CDmIOTTpBABEAiEHMYcmnQCASCDkIObQpBMAEAmEHMQcmnQCACKBkIOYQ5NOAEAkEHIQc2jSCQCIBEIOYk5mYOIxp6sAAN1AyEHMSecScgBABBByEHNo0gkAiARCDmIOTToBAJFAyEHMoUknACASCDmIOTTpBABEAiEHMYcmnQCASCDkICbRpBMA0F2EHMQkmnQCALqLkIOYRJNOAEB3EXIQk5iTAwDoLkIOYlIGTToBAN1EyEFMokknAKC7CDmISTTpBAB0FyEHMSmdOTkAgG4i5CAmZTInBwDQTYQcxCSadAIAuouQg5hEk04AQHcRchCTaNIJAOguQg5iEk06AQDdRchBzKJJJwCgOwg5iFk06QQAdAchBzGLJp0AgO4g5CBmMScHANAdhBzELJp0AgC6g5CDmEWTTgBAdxByELNo0gkA6A5CDmIWTToBAN1ByEHMokknAKA7CDmIWYFLyI9xJAcAcBEIOYhZzlNNOr0dfrW206QTANA1hBzErDR7ohKsnU063S1cYQUA6BpCDmKWxWKRIzlREiEHANB1XQo5ixcv1tVXX620tDRlZWVp+vTpqqqqCqlpbW1VaWmp+vXrp759++quu+5SXV1dSE1NTY2mTZum1NRUZWVl6YknnlBHR0dIzYYNG3TVVVfJbrdr5MiRWrZs2RnjWbp0qYYOHark5GQVFhZq27ZtXfk6iAPOlM5TVoQcAEBXdSnkbNy4UaWlpdqyZYvWrl2r9vZ2TZ06Vc3NzcGaxx57TL/73e+0YsUKbdy4UYcOHdKdd94Z3O7z+TRt2jS1tbVp8+bN+tWvfqVly5bp6aefDtZUV1dr2rRpuvHGG1VZWal58+bp/vvv13vvvResefPNNzV//nwtWrRIO3bs0IQJE1RSUqL6+vru7A/EmEDIaeCGgACArjLdUF9fbySZjRs3GmOMaWhoMElJSWbFihXBmk8++cRIMuXl5cYYY9555x1jtVqNy+UK1rz88svG4XAYr9drjDHmySefNGPGjAn5rBkzZpiSkpLg8ylTppjS0tLgc5/PZ3Jzc83ixYvDHr/b7TaSjNvt7sK3xqX0v3+xxeQvWGVWbK+N9lAAADEi3N/vbs3JcbvdkqTMzExJUkVFhdrb21VcXBysGT16tIYMGaLy8nJJUnl5ucaNG6fs7OxgTUlJiTwej/bs2ROsOf09AjWB92hra1NFRUVIjdVqVXFxcbDmbLxerzweT8iC2Ba4ISCnqwAAXXXRIcfv92vevHm69tprNXbsWEmSy+WSzWZTenp6SG12drZcLlew5vSAE9ge2Ha+Go/Ho5aWFh09elQ+n++sNYH3OJvFixfL6XQGl7y8vK5/cVxSzhQmHgMALs5Fh5zS0lLt3r1bb7zxRiTH06MWLlwot9sdXGpra6M9JFxAYE6Oh5ADAOiixIt50dy5c7Vq1Spt2rRJgwcPDq7PyclRW1ubGhoaQo7m1NXVKScnJ1jz51dBBa6+Or3mz6/Iqqurk8PhUEpKihISEpSQkHDWmsB7nI3dbpfdbu/6F0bUfDXxmLseAwC6pktHcowxmjt3rlauXKl169Zp2LBhIdsnTZqkpKQklZWVBddVVVWppqZGRUVFkqSioiLt2rUr5CqotWvXyuFwqKCgIFhz+nsEagLvYbPZNGnSpJAav9+vsrKyYA16h/QU5uQAAC5Ol47klJaW6rXXXtPbb7+ttLS04PwXp9OplJQUOZ1OzZkzR/Pnz1dmZqYcDoceeeQRFRUV6ZprrpEkTZ06VQUFBbr33nu1ZMkSuVwu/eAHP1BpaWnwKMtDDz2kF198UU8++aTuu+8+rVu3TsuXL9fq1auDY5k/f75mzZqlyZMna8qUKXr++efV3Nys2bNnR2rfIAY4uE8OAOBideWSLUlnXV599dVgTUtLi/n+979vMjIyTGpqqvn2t79tDh8+HPI+Bw4cMLfeeqtJSUkx/fv3N48//rhpb28PqVm/fr2ZOHGisdlsZvjw4SGfEfCzn/3MDBkyxNhsNjNlyhSzZcuWrnwdLiGPA5s/O2ryF6wyN/1/66M9FABAjAj399tijDHRi1jR5fF45HQ65Xa75XA4oj0cnMXeQx7d9tM/qn9fm7b/4JvRHg4AIAaE+/tN7yrEtPTUr05XXcZ5HABwEQg5iGmBq6vafUYt7b4ojwYAEE8IOYhpqbYEJVotkph8DADoGkIOYprFYqFJJwDgohByEPOcXEYOALgIhBzEPGcqIQcA0HWEHMQ8juQAAC4GIQcxjyadAICLQchBzGPiMQDgYhByEPPSOV0FALgIhBzEPJp0AgAuBiEHMY+JxwCAi0HIQcwLzskh5AAAuoCQg5jH1VUAgItByEHMS0+1SeJ0FQCgawg5iHmnz8kxxkR5NACAeEHIQcwLhByf36jJ2xHl0QAA4gUhBzEvOckqW0LnnyqnrAAA4SLkIOZZLBaadAIAuoyQg7jAvXIAAF1FyEFc4DJyAEBXEXIQF2jSCQDoKkIO4gJNOgEAXUXIQVygSScAoKsIOYgLTDwGAHQVIQdxgSadAICuIuQgLnB1FQCgqwg5iAvp3AwQANBFhBzEBebkAAC6ipCDuMB9cgAAXUXIQVwIzslpbZffb6I8GgBAPCDkIC4E7pNjjNTo7YjyaAAA8YCQg7iQnJSg5KTOP1eusAIAhIOQg7jB5GMAQFcQchA3mHwMAOgKQg7iRnqKTRJHcgAA4SHkIG7QpBMA0BWEHMQN5uQAALqCkIO48VWTzrYojwQAEA8IOYgbNOkEAHQFIQdxgyadAICuIOQgbjAnBwDQFYQcxA3ukwMA6Iouh5xNmzbp9ttvV25uriwWi956662Q7d/73vdksVhClltuuSWk5vjx45o5c6YcDofS09M1Z84cNTU1hdTs3LlT119/vZKTk5WXl6clS5acMZYVK1Zo9OjRSk5O1rhx4/TOO+909esgjnAJOQCgK7occpqbmzVhwgQtXbr0nDW33HKLDh8+HFxef/31kO0zZ87Unj17tHbtWq1atUqbNm3Sgw8+GNzu8Xg0depU5efnq6KiQs8995yeeeYZ/fznPw/WbN68Wffcc4/mzJmjjz76SNOnT9f06dO1e/furn4lxAnm5AAAusJijDEX/WKLRStXrtT06dOD6773ve+poaHhjCM8AZ988okKCgr04YcfavLkyZKkNWvW6LbbbtPBgweVm5url19+Wf/wD/8gl8slm63zLrdPPfWU3nrrLe3bt0+SNGPGDDU3N2vVqlXB977mmms0ceJEvfLKK2GN3+PxyOl0yu12y+FwXMQewKV0tMmryf/8B0nS/n+9TQlWS5RHBACIhnB/v3tkTs6GDRuUlZWlUaNG6eGHH9axY8eC28rLy5Wenh4MOJJUXFwsq9WqrVu3BmtuuOGGYMCRpJKSElVVVenEiRPBmuLi4pDPLSkpUXl5+TnH5fV65fF4QhbEj8CcHElqbOVoDgDg/CIecm655Rb993//t8rKyvRv//Zv2rhxo2699Vb5fD5JksvlUlZWVshrEhMTlZmZKZfLFazJzs4OqQk8v1BNYPvZLF68WE6nM7jk5eV178vikkpKsCrVliCJyccAgAtLjPQb3n333cHH48aN0/jx4zVixAht2LBBN998c6Q/rksWLlyo+fPnB597PB6CTpxxpiTpZJuPeTkAgAvq8UvIhw8frv79++uzzz6TJOXk5Ki+vj6kpqOjQ8ePH1dOTk6wpq6uLqQm8PxCNYHtZ2O32+VwOEIWxBfulQMACFePh5yDBw/q2LFjGjhwoCSpqKhIDQ0NqqioCNasW7dOfr9fhYWFwZpNmzapvf2rH7K1a9dq1KhRysjICNaUlZWFfNbatWtVVFTU018JUUTIAQCEq8shp6mpSZWVlaqsrJQkVVdXq7KyUjU1NWpqatITTzyhLVu26MCBAyorK9Mdd9yhkSNHqqSkRJJ05ZVX6pZbbtEDDzygbdu26YMPPtDcuXN19913Kzc3V5L0ne98RzabTXPmzNGePXv05ptv6oUXXgg51fToo49qzZo1+vGPf6x9+/bpmWee0fbt2zV37twI7BbEqq+adBJyAAAXYLpo/fr1RtIZy6xZs8zJkyfN1KlTzYABA0xSUpLJz883DzzwgHG5XCHvcezYMXPPPfeYvn37GofDYWbPnm0aGxtDaj7++GNz3XXXGbvdbgYNGmSeffbZM8ayfPlyc8UVVxibzWbGjBljVq9e3aXv4na7jSTjdru7uhsQJX+/vNLkL1hlXlz3abSHAgCIknB/v7t1n5x4x31y4s+/rN6r//xjtR68Ybj+z21XRns4AIAoiOp9coCeEpyTwyXkAIALIOQgrnw1J6ctyiMBAMQ6Qg7iCk06AQDhIuQgrqSndrb6cLd0RHkkAIBYR8hBXAmcrvJwJAcAcAGEHMQVbgYIAAgXIQdxJRBymrwdavf5ozwaAEAsI+QgrjiSv+opyykrAMD5EHIQVxITrEqzdwYdTlkBAM6HkIO4w2XkAIBwEHIQd2jSCQAIByEHcYfLyAEA4SDkIO6kp3K6CgBwYYQcxB2adAIAwkHIQdxhTg4AIByEHMQdrq4CAISDkIO4w5wcAEA4CDmIO/SvAgCEg5CDuMMl5ACAcBByEHeCE4+5ugoAcB6EHMQdTlcBAMJByEHcSU+xSZJa2n1q6/BHeTQAgFhFyEHcSUtOlMXS+ZijOQCAcyHkIO5YrRal2RMlSe6WtiiPBgAQqwg5iEtO7pUDALgAQg7iUmBeDiEHAHAuhBzEJa6wAgBcCCEHcYlO5ACACyHkIC456EQOALgAQg7iEqerAAAXQshBXKITOQDgQgg5iEs06QQAXAghB3GJJp0AgAsh5CAuMScHAHAhhBzEJUIOAOBCCDmIS4QcAMCFEHIQlwK9q7wdfrW2+6I8GgBALCLkIC71tSXKaul8zNEcAMDZEHIQl6xWC6esAADnRchB3CLkAADOh5CDuEWTTgDA+RByELdo0gkAOB9CDuIWp6sAAOfT5ZCzadMm3X777crNzZXFYtFbb70Vst0Yo6effloDBw5USkqKiouL9emnn4bUHD9+XDNnzpTD4VB6errmzJmjpqamkJqdO3fq+uuvV3JysvLy8rRkyZIzxrJixQqNHj1aycnJGjdunN55552ufh3EMZp0AgDOp8shp7m5WRMmTNDSpUvPun3JkiX66U9/qldeeUVbt25Vnz59VFJSotbW1mDNzJkztWfPHq1du1arVq3Spk2b9OCDDwa3ezweTZ06Vfn5+aqoqNBzzz2nZ555Rj//+c+DNZs3b9Y999yjOXPm6KOPPtL06dM1ffp07d69u6tfCXGKJp0AgPMy3SDJrFy5Mvjc7/ebnJwc89xzzwXXNTQ0GLvdbl5//XVjjDF79+41ksyHH34YrHn33XeNxWIxX375pTHGmJdeeslkZGQYr9cbrFmwYIEZNWpU8Pnf/M3fmGnTpoWMp7Cw0Pzt3/5t2ON3u91GknG73WG/BrHjPzZ+ZvIXrDKPvr4j2kMBAFxC4f5+R3ROTnV1tVwul4qLi4PrnE6nCgsLVV5eLkkqLy9Xenq6Jk+eHKwpLi6W1WrV1q1bgzU33HCDbDZbsKakpERVVVU6ceJEsOb0zwnUBD7nbLxerzweT8iC+MWcHADA+UQ05LhcLklSdnZ2yPrs7OzgNpfLpaysrJDtiYmJyszMDKk523uc/hnnqglsP5vFixfL6XQGl7y8vK5+RcQQZ0pnCCbkAADO5rK6umrhwoVyu93Bpba2NtpDQjdwJAcAcD4RDTk5OTmSpLq6upD1dXV1wW05OTmqr68P2d7R0aHjx4+H1JztPU7/jHPVBLafjd1ul8PhCFkQvwg5AIDziWjIGTZsmHJyclRWVhZc5/F4tHXrVhUVFUmSioqK1NDQoIqKimDNunXr5Pf7VVhYGKzZtGmT2tu/+vFau3atRo0apYyMjGDN6Z8TqAl8Dno/52mXkBtjojwaAECs6XLIaWpqUmVlpSorKyV1TjaurKxUTU2NLBaL5s2bp3/+53/Wb3/7W+3atUvf/e53lZubq+nTp0uSrrzySt1yyy164IEHtG3bNn3wwQeaO3eu7r77buXm5kqSvvOd78hms2nOnDnas2eP3nzzTb3wwguaP39+cByPPvqo1qxZox//+Mfat2+fnnnmGW3fvl1z587t/l5BXAgcyWn3GbW0+6I8GgBAzOnqZVvr1683ks5YZs2aZYzpvIz8hz/8ocnOzjZ2u93cfPPNpqqqKuQ9jh07Zu655x7Tt29f43A4zOzZs01jY2NIzccff2yuu+46Y7fbzaBBg8yzzz57xliWL19urrjiCmOz2cyYMWPM6tWru/RduIQ8vvn9fjNi4WqTv2CVOdRwMtrDAQBcIuH+fluMuXyP83s8HjmdTrndbubnxKlJ/7RWx5rbtGbe9Rqdw/+GAHA5CPf3+7K6ugq9T+CUVQOdyAEAf4aQg7jm4AorAMA5EHIQ12jSCQA4F0IO4hpNOgEA50LIQVxjTg4A4FwIOYhr3PUYAHAuhBzENUIOAOBcCDmIa4QcAMC5EHIQ14Jzcgg5AIA/Q8hBXOPqKgDAuRByENec3CcHAHAOhBzEtfQUm6TOkHMZt2EDAJwFIQdxLXC6yuc3am7zRXk0AIBYkhjtAQDdkZxklS3BqjafXx8eOK5cZ4qSEixKSrDKlmhVUoJViQkW2RI6H1stksViifawAQCXACEHcc1isciZmqQjjV7NfvXDsF5jtUiJVqusVinBYlGCNbBYlXBqneXUeqtFslotslosp9br1HpLMDBZLTr1vHO71WKR1frVusD2kFpr4LWhrz9z+9nHkxCot35VYz31PRID47V2rksM1AceWy1KSuj8vomnvSbBalFigkWJ1s5AGAiLSYlWJVm/epxo7QyNVithEUBsI+Qg7s25bph+vfULtXcYtfv8avP51e7zq8Nn1OE/c56O30htPr/E2a1uSUqwyJ6YIHuitXNJ6nxsCzxPTFCKLUGpwSVRqbZT65JOPbcnKC05SY7kRDlSkuRMSZIjOUm2RM6kA+g+i7mMZ2t6PB45nU653W45HI5oDwc9wO83avf71e4zau/wy2eM/P7O8OMLLOa0x34jvzHym855PsYE1qnz8annRqee+yW/6azzGwVf6z/tfYwxMqdvO60+8HmB7b7A41OfGRjv6a8NjvHPagLfxe838p16jw6/PzjGwPfrXNe5DwJB0Of3f/Xc51e73wSDYiA0Xsr/UiQnWeVITpIjpTMAZfaxa0CaTQP62tU/zR7y74A0u/rY+f/XgMtJuL/f/JcBvZrVapHdmiB7oiR7tEcT33yngk+7z6+2js4jZt52v7wdfnk7fGrr+Oqxt92v1g6fTrb51NLW+W/n0nHaug41t/nU2NohT0u7PK3tamztkCS1tvvV2u5VfaM3rLGlJCUox5msvMxU5WWkaEhm6qnHqRqSmRq81QCAywshB0BYOuctJSg5KaHHPsPnN2pq7ZCntV3uU8HH09KuY81tOtLo1dEm76l/O58fafSqpd2nlnafqo82q/po81nfNy05UUMyU5XfL1VXZKdpdE6arshOU36/PkpgbhHQa3G6itNVQFxr9nboSKNXh9wtqj1+UrXHW1Rz/KRqT5xU7fGTOtrUds7XJidZ9bWstK+CT06arsxJU5Yj+RJ+AwBdxekqAJeFPvZE9bEnamj/PtKIM7efbOvQwRMtqjl2Up8fbVKVq0l/qmvUn+oa1dru164v3dr1pTvkNdkOuyYMTteEvHRNGJyucYOdwXsyAYgfHMnhSA5wWfL5jWqOn1SVy6MqV5Oq6jyqcjWq+mizznJRnoYP6NMZfAY7NSEvXQW5DtkTe+7UHYBzC/f3m5BDyAFwmpNtHdpzyKOPaxtUWdugjw82qPZ4yxl1tkSrxg9yatLQDE0akqFJ+Rnq15fZ7cClQMgJAyEHQDiON7fp44MN+ri2QTsPulVZ26DjzWfO9RnWv4+uGpKhyUM7Q8/IAX25aSLQAwg5YSDkALgYxhgdOHZS2w8c146aE9p+4IQ+rW86o86RnKhJ+RmaPDRTk/MzNCEvvUevTgMuF4ScMBByAESK+2S7dtSeUMWBE6r44oQqaxvU0h56W+2kBIvGDnJq8mnBh1NcQNcRcsJAyAHQU9p9fn1y2KMPD5xQxRfHtf3AibPe3HBY/z6dR3vyO09zjRjQlyaywAUQcsJAyAFwqRhjdPBEiz48cFzbv+g84lNV13hGXXpqUudE5qEZmpyfqfGDnZziAv4MIScMhBwA0eQ+2d45p+fUkZ6PDzaotd0fUhM4xTXp1ITmq/IzlJXGzQpxeSPkhIGQAyCWtPv82nPIo+0HjqviixPa/sUJHTnLKa4hmamalN8ZeCbnZ+iK7DTaU+CyQsgJAyEHQCwzxqj2eIsqajqP9FR80XmK68//q51mT9TEIem6emimJg/N0MS8dKXauKE9ei9CThgIOQDijae1XZU1Ddr+xQnt+OKEPqo5oea20Ku4Eq0WjRnk1NWBq7iGZqg/V3GhFyHkhIGQAyDedfj8qqpr1PYDnae3Pqw+Lpen9Yy64f37aPLQDBUO66eiEf2Um54ShdECkUHICQMhB0BvY4zRlw0t2n7gROeVXOe4iiu/X6qKhncGnqLh/ei8jrhCyAkDIQfA5cB9sl0VNce1rfqEyj8/pl0HG85oQjp8QB/9xYh+KhreX38xop8y+tiiM1ggDIScMBByAFyOGlvb9eGB4yrff0yb9x/T3sOekMnMVos0MS9dN47K0o2js1Qw0EEPLsQUQk4YCDkAIDWcbNPW6s7QU77/2Bmntwak2fWXVwzQjaOzdN3X+suRnBSlkQKdCDlhIOQAwJkONbRoQ9URra+q1wefHdXJ067eSrRaNCk/Q8VXZuuWsTnKy0yN4khxuSLkhIGQAwDn5+3w6cPqE1pfVa/1VfX6/EhzyPbxg526bdxA3TZ2oIb0I/Dg0iDkhIGQAwBd88WxZq3fV681e1zaVn08ZALz2EEO3TZuoKaNG6j8fn2iN0j0eoScMBByAODiHWn06r09Lr2z67C2fH4sJPAUDHRo2viBumNirgZncIQHkUXICQMhBwAi41iTV+/tqdM7uw6r/PNj8p2WeK4Znqk7vz5Yt47LURqTlhEBhJwwEHIAIPKON7fp93tcervykMo/PxZcb0+0qmRMjr591SBdP7K/EhOsURwl4lm4v98R/wt75plnZLFYQpbRo0cHt7e2tqq0tFT9+vVT3759ddddd6muri7kPWpqajRt2jSlpqYqKytLTzzxhDo6OkJqNmzYoKuuukp2u10jR47UsmXLIv1VAAAXIbOPTXdPGaLXH7xGHzx1k54oGaURA/rI2+HXbz8+pNmvfqhrFq/TP63aqz2H3NEeLnqxHmlTO2bMGP3hD3/46kMSv/qYxx57TKtXr9aKFSvkdDo1d+5c3Xnnnfrggw8kST6fT9OmTVNOTo42b96sw4cP67vf/a6SkpL0r//6r5Kk6upqTZs2TQ899JB+/etfq6ysTPfff78GDhyokpKSnvhKAICLMCg9RaU3jtT3/3KEdh50a+VHX+q3Hx/S0Sav/uv9av3X+9UaO8ihe6YM0V9NyOV0FiIq4qernnnmGb311luqrKw8Y5vb7daAAQP02muv6a//+q8lSfv27dOVV16p8vJyXXPNNXr33Xf1rW99S4cOHVJ2drYk6ZVXXtGCBQt05MgR2Ww2LViwQKtXr9bu3buD73333XeroaFBa9asCXusnK4CgEuvrcOvjX86opUfHdQf9tarzeeXJKXaEvRXE3J195QhmjDYKYuFuyzj7KJ2ukqSPv30U+Xm5mr48OGaOXOmampqJEkVFRVqb29XcXFxsHb06NEaMmSIysvLJUnl5eUaN25cMOBIUklJiTwej/bs2ROsOf09AjWB9zgXr9crj8cTsgAALi1bolXfLMjWSzMnacv/uVk/mHalhg/oo5NtPr3xYa2mL/1At/30ff3f8gPytLZHe7iIYxEPOYWFhVq2bJnWrFmjl19+WdXV1br++uvV2Ngol8slm82m9PT0kNdkZ2fL5XJJklwuV0jACWwPbDtfjcfjUUtLyznHtnjxYjmdzuCSl5fX3a8LAOiGzD423X/9cJXN/4aW/22Rvv31QbIlWvXJYY9++PYeTfmXP+jvV3ysnQcboj1UxKGIz8m59dZbg4/Hjx+vwsJC5efna/ny5UpJSYn0x3XJwoULNX/+/OBzj8dD0AGAGGCxWDRlWKamDMvUotsLtPKjL/X6thr9qa5J/1NxUP9TcVCT8zN033XDNLUgmyuzEJYemXh8uvT0dF1xxRX67LPP9M1vflNtbW1qaGgIOZpTV1ennJwcSVJOTo62bdsW8h6Bq69Or/nzK7Lq6urkcDjOG6TsdrvsdnskvhYAoIekp9o0+9ph+t5fDNWOmhP6/7fUaNXOQ9r+xQlt/+KEBqWnaNZf5GvG1UPkTGGiMs6tx6NwU1OT9u/fr4EDB2rSpElKSkpSWVlZcHtVVZVqampUVFQkSSoqKtKuXbtUX18frFm7dq0cDocKCgqCNae/R6Am8B4AgPhnsVg0KT9T/z5joj5YcJP+7qaRyuxj05cNLfrXd/apaHGZnn57tz4/0hTtoSJGRfzqqr//+7/X7bffrvz8fB06dEiLFi1SZWWl9u7dqwEDBujhhx/WO++8o2XLlsnhcOiRRx6RJG3evFlS5yXkEydOVG5urpYsWSKXy6V7771X999/f8gl5GPHjlVpaanuu+8+rVu3Tn/3d3+n1atXd+kScq6uAoD40tru028rD+mXH1Rrn6sxuP6m0Vm6//phKhrej6uyLgNRu+Px3XffrU2bNunYsWMaMGCArrvuOv3Lv/yLRowYIanzZoCPP/64Xn/9dXm9XpWUlOill14KnoqSpC+++EIPP/ywNmzYoD59+mjWrFl69tlnQ+63s2HDBj322GPau3evBg8erB/+8If63ve+16WxEnIAID4ZY7R5/zH98v1qle376sj/pPwMzb1xpP5y1ADCTi9GW4cwEHIAIP59fqRJv/ygWsu3H1RbR+c9d8bkOvTITSM1tSBHVithp7ch5ISBkAMAvUe9p1X/+cfP9eutNTrZ5pMkfS2rr0pvHKlvjR/IFVm9CCEnDIQcAOh9jje36dUPqrXsgwNq9Hb2Pczvl6qHvzFCd141WLZEwk68I+SEgZADAL2Xp7Vd/7f8C/3ij5/rxMnOOyfnOpP1/RtH6n9NHix7YkKUR4iLRcgJAyEHAHq/k20dem1rjX6+6XPVN3oldYad0ptG6n9NyuPIThwi5ISBkAMAl4/Wdp/e2FajlzbsD4adQekp+v6NIwg7cYaQEwZCDgBcfgg78Y+QEwZCDgBcvlrbfXp9W41ePkvY+etJzNmJZYScMBByAABnCztZaXbNuW6YvlM4RGnJ9MeKNYScMBByAAABre0+vba1Rv/5x8912N0qSXIkJ+q7RUP1vWuHqn9fGjzHCkJOGAg5AIA/19bh11uVX+qVjfv1+ZFmSZI90aoZV+fpgeuHKy8zNcojBCEnDIQcAMC5+PxGa/e69NKG/dp50C1JSrBa9FcTcvW33xiu0Tn8bkQLIScMhBwAwIUYY1S+/5he2rBf7392NLj+2pH9dO81+Sq+MpuWEZcYIScMhBwAQFfsPNigVzbu17u7XQr8emY77LpnyhDdM2WIsh3J0R3gZYKQEwZCDgDgYtQeP6nXt9XozQ9rday5TVLnqaypBdm695p8FY3oJ4uF7uc9hZATBkIOAKA7vB0+rdnt0q+31GjbgePB9SMG9NHMwnzdedUgpafaojjC3omQEwZCDgAgUva5PPr1lhr9ZsdBNbf5JElJCRbdOCpLd141SDeOzuIGgxFCyAkDIQcAEGlN3g699dGXem1rjfYe9gTXO1OSNG38QN359UGalJ/B6axuIOSEgZADAOhJVa5G/eajg3r7o0NyeVqD6/MyU/TtiYP07asGa1j/PlEcYXwi5ISBkAMAuBR8fqMtnx/Tb3Z8qTW7DwdPZ0nS+MFO3TZuoG4bO1BD+nGjwXAQcsJAyAEAXGotbT79fq9Lv9nxpf746RH5T/sVHjvIEQw8QznCc06EnDAQcgAA0XSk0av39rj07u7DKt9/LCTwFAx06LZxObpt3EANH9A3eoOMQYScMBByAACx4liTV+/tqdO7uw9r8/5j8p2WeEZlp2nqmGxNLcjR2EGOy37SMiEnDIQcAEAsOt7cprV7XXpnl0sffHZUHacFnlxnsr5ZkK2pY3I0ZVimki7DlhKEnDAQcgAAsa7hZJvW7avX7/fUaeOfjqil/atJy47kRN00OktTx+TohisGqK89MYojvXQIOWEg5AAA4klru08ffHZUv99Tpz98UhdsKSFJtgSrrhnRTzePztJNo7OUl9l7r9Qi5ISBkAMAiFc+v9GOmhP6/R6Xfr+3Tl8cOxmy/YrsvrppdLaKr8zS14dkKMHae+bxEHLCQMgBAPQGxhjtP9Kksk/qVbavXtsPHA+5Uis9NUk3jsrSjaOzdP3I/sroE9/9tAg5YSDkAAB6o4aTbdr4pyMq+6ReG6rq5WntCG6zWKTxg9N1w9f664YrBmhiXnrcTV4m5ISBkAMA6O06fH5VfHFC66rqtX5fvf5U1xSyva89UUUj+umGKwbohq/1V36/2L8JISEnDIQcAMDlxuVu1R8/PaJNnx7V+58e0YmT7SHb8/ulqmh4P00ZlqnC4f00KD0lSiM9N0JOGAg5AIDLmd9vtOeQR5s+PaJNfzqiii9OhNyTR5IGpaeocFhmMPQM7Zca9ZsREnLCQMgBAOArTd4Obf38mLZVH9fW6uPa9aU75M7LkjQgza4pwzI1OT9DE/PSVZDrkD0x4ZKOk5ATBkIOAADn1uzt0I6aE52h5/PjqqxtUJvPH1JjS7DqylyHvp6Xrq8PSdfEvHQNyezZoz2EnDAQcgAACF9ru08f1zZoW/Vx7ag5ocrahjPm9EhSZh+bJgx2amJehu4pzFNWWnJEx0HICQMhBwCAi2eMUc3xk6qsbdBHNQ2qrG3Q3kOekKM9m5+6SbkRnrwc7u/35dHkAgAARJzFYlF+vz7K79dHd0wcJEnydvj0yeFGVdac0GdHmjTQGdmjOF1ByAEAABFjT0zQxLzOuTnRFl+3OAQAAAgTIQcAAPRKhBwAANArEXIAAECvRMgBAAC9EiEHAAD0SnEfcpYuXaqhQ4cqOTlZhYWF2rZtW7SHBAAAYkBch5w333xT8+fP16JFi7Rjxw5NmDBBJSUlqq+vj/bQAABAlMV1yPnJT36iBx54QLNnz1ZBQYFeeeUVpaam6pe//GW0hwYAAKIsbkNOW1ubKioqVFxcHFxntVpVXFys8vLys77G6/XK4/GELAAAoHeK25Bz9OhR+Xw+ZWdnh6zPzs6Wy+U662sWL14sp9MZXPLy8i7FUAEAQBTEbci5GAsXLpTb7Q4utbW10R4SAADoIXHboLN///5KSEhQXV1dyPq6ujrl5OSc9TV2u112u/1SDA8AAERZ3IYcm82mSZMmqaysTNOnT5ck+f1+lZWVae7cuWG9hzFGkpibAwBAHAn8bgd+x88lbkOOJM2fP1+zZs3S5MmTNWXKFD3//PNqbm7W7Nmzw3p9Y2OjJDE3BwCAONTY2Cin03nO7XEdcmbMmKEjR47o6aeflsvl0sSJE7VmzZozJiOfS25urmpra5WWliaLxRKxcXk8HuXl5am2tlYOhyNi74uzY39fWuzvS4v9fWmxvy+ti93fxhg1NjYqNzf3vHUWc6FjPegyj8cjp9Mpt9vN/5FcAuzvS4v9fWmxvy8t9vel1dP7+7K6ugoAAFw+CDkAAKBXIuT0ALvdrkWLFnG5+iXC/r602N+XFvv70mJ/X1o9vb+ZkwMAAHoljuQAAIBeiZADAAB6JUIOAADolQg5AACgVyLk9IClS5dq6NChSk5OVmFhobZt2xbtIfUKmzZt0u23367c3FxZLBa99dZbIduNMXr66ac1cOBApaSkqLi4WJ9++ml0BhvnFi9erKuvvlppaWnKysrS9OnTVVVVFVLT2tqq0tJS9evXT3379tVdd911RsNchO/ll1/W+PHj5XA45HA4VFRUpHfffTe4nf3dc5599llZLBbNmzcvuI79HVnPPPOMLBZLyDJ69Ojg9p7a34ScCHvzzTc1f/58LVq0SDt27NCECRNUUlKi+vr6aA8t7jU3N2vChAlaunTpWbcvWbJEP/3pT/XKK69o69at6tOnj0pKStTa2nqJRxr/Nm7cqNLSUm3ZskVr165Ve3u7pk6dqubm5mDNY489pt/97ndasWKFNm7cqEOHDunOO++M4qjj2+DBg/Xss8+qoqJC27dv10033aQ77rhDe/bskcT+7ikffvih/uM//kPjx48PWc/+jrwxY8bo8OHDweX9998Pbuux/W0QUVOmTDGlpaXB5z6fz+Tm5prFixdHcVS9jySzcuXK4HO/329ycnLMc889F1zX0NBg7Ha7ef3116Mwwt6lvr7eSDIbN240xnTu26SkJLNixYpgzSeffGIkmfLy8mgNs9fJyMgwv/jFL9jfPaSxsdF87WtfM2vXrjXf+MY3zKOPPmqM4e+7JyxatMhMmDDhrNt6cn9zJCeC2traVFFRoeLi4uA6q9Wq4uJilZeXR3FkvV91dbVcLlfIvnc6nSosLGTfR4Db7ZYkZWZmSpIqKirU3t4esr9Hjx6tIUOGsL8jwOfz6Y033lBzc7OKiorY3z2ktLRU06ZNC9mvEn/fPeXTTz9Vbm6uhg8frpkzZ6qmpkZSz+7vuO5CHmuOHj0qn893Rhf07Oxs7du3L0qjujy4XC5JOuu+D2zDxfH7/Zo3b56uvfZajR07VlLn/rbZbEpPTw+pZX93z65du1RUVKTW1lb17dtXK1euVEFBgSorK9nfEfbGG29ox44d+vDDD8/Yxt935BUWFmrZsmUaNWqUDh8+rB/96Ee6/vrrtXv37h7d34QcAOdVWlqq3bt3h5w/R88YNWqUKisr5Xa79T//8z+aNWuWNm7cGO1h9Tq1tbV69NFHtXbtWiUnJ0d7OJeFW2+9Nfh4/PjxKiwsVH5+vpYvX66UlJQe+1xOV0VQ//79lZCQcMaM8Lq6OuXk5ERpVJeHwP5l30fW3LlztWrVKq1fv16DBw8Ors/JyVFbW5saGhpC6tnf3WOz2TRy5EhNmjRJixcv1oQJE/TCCy+wvyOsoqJC9fX1uuqqq5SYmKjExERt3LhRP/3pT5WYmKjs7Gz2dw9LT0/XFVdcoc8++6xH/74JORFks9k0adIklZWVBdf5/X6VlZWpqKgoiiPr/YYNG6acnJyQfe/xeLR161b2/UUwxmju3LlauXKl1q1bp2HDhoVsnzRpkpKSkkL2d1VVlWpqatjfEeT3++X1etnfEXbzzTdr165dqqysDC6TJ0/WzJkzg4/Z3z2rqalJ+/fv18CBA3v277tb05ZxhjfeeMPY7XazbNkys3fvXvPggw+a9PR043K5oj20uNfY2Gg++ugj89FHHxlJ5ic/+Yn56KOPzBdffGGMMebZZ5816enp5u233zY7d+40d9xxhxk2bJhpaWmJ8sjjz8MPP2ycTqfZsGGDOXz4cHA5efJksOahhx4yQ4YMMevWrTPbt283RUVFpqioKIqjjm9PPfWU2bhxo6murjY7d+40Tz31lLFYLOb3v/+9MYb93dNOv7rKGPZ3pD3++ONmw4YNprq62nzwwQemuLjY9O/f39TX1xtjem5/E3J6wM9+9jMzZMgQY7PZzJQpU8yWLVuiPaReYf369UbSGcusWbOMMZ2Xkf/whz802dnZxm63m5tvvtlUVVVFd9Bx6mz7WZJ59dVXgzUtLS3m+9//vsnIyDCpqanm29/+tjl8+HD0Bh3n7rvvPpOfn29sNpsZMGCAufnmm4MBxxj2d0/785DD/o6sGTNmmIEDBxqbzWYGDRpkZsyYYT777LPg9p7a3xZjjOnesSAAAIDYw5wcAADQKxFyAABAr0TIAQAAvRIhBwAA9EqEHAAA0CsRcgAAQK9EyAEAAL0SIQcAAPRKhBwAANArEXIAAECvRMgBAAC9EiEHAAD0Sv8PU09bdIyMxWoAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "%%time\n",
        "tensor0 = data_w_noise  # Исходный тензор\n",
        "\n",
        "core, fac, history = train(tensor0, [10,10,10], lr=1e-3, epochs=5000)\n",
        "\n",
        "result = rebuild_tensor(core, fac)\n",
        "\n",
        "mse = torch.nn.functional.mse_loss(result, tensor0)\n",
        "print(f\"MSE после градиентного разложения: {mse.item():.6e}\")\n",
        "plt.plot(np.arange(50), history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za8JKgR-Falk"
      },
      "source": [
        "## 5 Приведите сравнение скорости работы и ошибки восстановления методом из пакета и реализованного градиентного\n",
        "Сравнение может считаться ± объективным с размером выборки от 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0zL8xRQQc4e",
        "outputId": "e1158bc8-5d5a-41f2-d795-0d02060e0aed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100, Loss: 22013.95900883345\n",
            "Epoch 200, Loss: 9400.320723011215\n",
            "Epoch 300, Loss: 8893.666312412897\n",
            "Epoch 400, Loss: 7121.307639639073\n",
            "Epoch 500, Loss: 5193.364482478751\n",
            "Epoch 600, Loss: 3471.4402468055578\n",
            "Epoch 700, Loss: 2254.045515292714\n",
            "Epoch 800, Loss: 1421.5714115973733\n",
            "Epoch 900, Loss: 789.7308466377748\n",
            "Epoch 1000, Loss: 476.51213969927386\n",
            "Epoch 1100, Loss: 343.3315774263849\n",
            "Epoch 1200, Loss: 256.309417590574\n",
            "Epoch 1300, Loss: 186.78548746814346\n",
            "Epoch 1400, Loss: 132.13040222209122\n",
            "Epoch 1500, Loss: 91.7482507605299\n",
            "Epoch 1600, Loss: 63.31421394768722\n",
            "Epoch 1700, Loss: 43.90744448833822\n",
            "Epoch 1800, Loss: 30.85510620107311\n",
            "Epoch 1900, Loss: 22.071179500634468\n",
            "Epoch 2000, Loss: 16.088511366489517\n",
            "Epoch 2100, Loss: 11.937947512606687\n",
            "Epoch 2200, Loss: 8.998120763018395\n",
            "Epoch 2300, Loss: 6.8732066684020605\n",
            "Epoch 2400, Loss: 5.308853950157805\n",
            "Epoch 2500, Loss: 4.138726425347272\n",
            "Epoch 2600, Loss: 3.251639673216895\n",
            "Epoch 2700, Loss: 2.5715352983779307\n",
            "Epoch 2800, Loss: 2.0452052259870013\n",
            "Epoch 2900, Loss: 1.6346551341650337\n",
            "Epoch 3000, Loss: 1.3122588712517975\n",
            "Epoch 3100, Loss: 1.057614115290585\n",
            "Epoch 3200, Loss: 0.8554538444546422\n",
            "Epoch 3300, Loss: 0.6942268991944491\n",
            "Epoch 3400, Loss: 0.5651121958882884\n",
            "Epoch 3500, Loss: 0.46132051802360985\n",
            "Epoch 3600, Loss: 0.3775913452007256\n",
            "Epoch 3700, Loss: 0.3098247872026898\n",
            "Epoch 3800, Loss: 0.25480892643089503\n",
            "Epoch 3900, Loss: 0.21001568989913585\n",
            "Epoch 4000, Loss: 0.17344666851568927\n",
            "Epoch 4100, Loss: 0.14351579143583448\n",
            "Epoch 4200, Loss: 0.11895947608351531\n",
            "Epoch 4300, Loss: 0.09876743753579828\n",
            "Epoch 4400, Loss: 0.08212914378863564\n",
            "Epoch 4500, Loss: 0.0683921925045318\n",
            "Epoch 4600, Loss: 0.0570298195909829\n",
            "Epoch 4700, Loss: 0.04761543574031712\n",
            "Epoch 4800, Loss: 0.03980259505091617\n",
            "Epoch 4900, Loss: 0.03330917911809498\n",
            "Epoch 5000, Loss: 0.027904864993135534\n",
            "Epoch 100, Loss: 25610.274106022094\n",
            "Epoch 200, Loss: 8674.118479422563\n",
            "Epoch 300, Loss: 8153.187537384797\n",
            "Epoch 400, Loss: 7012.781863870149\n",
            "Epoch 500, Loss: 5524.365842874798\n",
            "Epoch 600, Loss: 3853.5011045069796\n",
            "Epoch 700, Loss: 2313.6889880756567\n",
            "Epoch 800, Loss: 1498.9359050052913\n",
            "Epoch 900, Loss: 1010.8421178083231\n",
            "Epoch 1000, Loss: 675.920884464651\n",
            "Epoch 1100, Loss: 466.77187716527317\n",
            "Epoch 1200, Loss: 353.2922110199563\n",
            "Epoch 1300, Loss: 274.26279032239245\n",
            "Epoch 1400, Loss: 204.97073218293525\n",
            "Epoch 1500, Loss: 142.97284269085117\n",
            "Epoch 1600, Loss: 92.22751908123031\n",
            "Epoch 1700, Loss: 56.053283878894\n",
            "Epoch 1800, Loss: 33.36609240434645\n",
            "Epoch 1900, Loss: 20.010887146836268\n",
            "Epoch 2000, Loss: 12.202624994610805\n",
            "Epoch 2100, Loss: 7.565409663076093\n",
            "Epoch 2200, Loss: 4.7590236134897435\n",
            "Epoch 2300, Loss: 3.0320285593628244\n",
            "Epoch 2400, Loss: 1.9540110661099224\n",
            "Epoch 2500, Loss: 1.2726462509010654\n",
            "Epoch 2600, Loss: 0.8371081406264803\n",
            "Epoch 2700, Loss: 0.5557964581767383\n",
            "Epoch 2800, Loss: 0.3723232783197592\n",
            "Epoch 2900, Loss: 0.251558178909392\n",
            "Epoch 3000, Loss: 0.17137320609896017\n",
            "Epoch 3100, Loss: 0.11768770994190621\n",
            "Epoch 3200, Loss: 0.08145585727583349\n",
            "Epoch 3300, Loss: 0.05681389937299359\n",
            "Epoch 3400, Loss: 0.03992840846396248\n",
            "Epoch 3500, Loss: 0.028273029741497244\n",
            "Epoch 3600, Loss: 0.0201699957393588\n",
            "Epoch 3700, Loss: 0.014496875901034859\n",
            "Epoch 3800, Loss: 0.010497428101544054\n",
            "Epoch 3900, Loss: 0.007658624538497058\n",
            "Epoch 4000, Loss: 0.005630097716375734\n",
            "Epoch 4100, Loss: 0.004170995580232311\n",
            "Epoch 4200, Loss: 0.0031146797744569113\n",
            "Epoch 4300, Loss: 0.002345122315273402\n",
            "Epoch 4400, Loss: 0.0017810239783594209\n",
            "Epoch 4500, Loss: 0.0013650625049011472\n",
            "Epoch 4600, Loss: 0.0010565699235969047\n",
            "Epoch 4700, Loss: 0.0008265158951197827\n",
            "Epoch 4800, Loss: 0.0006540508698322533\n",
            "Epoch 4900, Loss: 0.0005241103341619365\n",
            "Epoch 5000, Loss: 0.00042574492415606557\n",
            "Epoch 100, Loss: 28845.800262104258\n",
            "Epoch 200, Loss: 8245.837998060524\n",
            "Epoch 300, Loss: 7708.101469509143\n",
            "Epoch 400, Loss: 6756.941695988138\n",
            "Epoch 500, Loss: 5471.696462460166\n",
            "Epoch 600, Loss: 3964.3070506117047\n",
            "Epoch 700, Loss: 2915.0944295045038\n",
            "Epoch 800, Loss: 2162.968953117021\n",
            "Epoch 900, Loss: 1640.9129811030302\n",
            "Epoch 1000, Loss: 1249.2387403471812\n",
            "Epoch 1100, Loss: 907.5999377790856\n",
            "Epoch 1200, Loss: 649.0941554673743\n",
            "Epoch 1300, Loss: 476.6545928326911\n",
            "Epoch 1400, Loss: 357.0697190314931\n",
            "Epoch 1500, Loss: 270.5735736036676\n",
            "Epoch 1600, Loss: 203.75199991947557\n",
            "Epoch 1700, Loss: 149.04600600529793\n",
            "Epoch 1800, Loss: 105.70968949112113\n",
            "Epoch 1900, Loss: 73.7986552512516\n",
            "Epoch 2000, Loss: 51.36207275236256\n",
            "Epoch 2100, Loss: 35.84396420242006\n",
            "Epoch 2200, Loss: 25.14809172054271\n",
            "Epoch 2300, Loss: 17.7712943555766\n",
            "Epoch 2400, Loss: 12.668568273301295\n",
            "Epoch 2500, Loss: 9.119102451807446\n",
            "Epoch 2600, Loss: 6.6299308762822085\n",
            "Epoch 2700, Loss: 4.86696154679747\n",
            "Epoch 2800, Loss: 3.6049445033097247\n",
            "Epoch 2900, Loss: 2.6919174089300806\n",
            "Epoch 3000, Loss: 2.0247552938978846\n",
            "Epoch 3100, Loss: 1.5328025402927508\n",
            "Epoch 3200, Loss: 1.1670859841357029\n",
            "Epoch 3300, Loss: 0.8932432937748248\n",
            "Epoch 3400, Loss: 0.6868740719616448\n",
            "Epoch 3500, Loss: 0.5304581359390467\n",
            "Epoch 3600, Loss: 0.41128864190426284\n",
            "Epoch 3700, Loss: 0.3200672501772356\n",
            "Epoch 3800, Loss: 0.24993612862398956\n",
            "Epoch 3900, Loss: 0.19580213947523437\n",
            "Epoch 4000, Loss: 0.1538592773832571\n",
            "Epoch 4100, Loss: 0.12124752828572945\n",
            "Epoch 4200, Loss: 0.09580684217780196\n",
            "Epoch 4300, Loss: 0.0758982108607124\n",
            "Epoch 4400, Loss: 0.06027258880443603\n",
            "Epoch 4500, Loss: 0.04797423875946985\n",
            "Epoch 4600, Loss: 0.03826904654364898\n",
            "Epoch 4700, Loss: 0.030591074463605053\n",
            "Epoch 4800, Loss: 0.024502520305124207\n",
            "Epoch 4900, Loss: 0.019663584710986618\n",
            "Epoch 5000, Loss: 0.015809699354589986\n",
            "Epoch 100, Loss: 26833.325745717193\n",
            "Epoch 200, Loss: 6828.868452060703\n",
            "Epoch 300, Loss: 6617.416226876709\n",
            "Epoch 400, Loss: 6178.535869007736\n",
            "Epoch 500, Loss: 5629.151515787782\n",
            "Epoch 600, Loss: 4964.200024187827\n",
            "Epoch 700, Loss: 4102.680516029164\n",
            "Epoch 800, Loss: 2993.37195222418\n",
            "Epoch 900, Loss: 2032.813566543594\n",
            "Epoch 1000, Loss: 1404.618965526803\n",
            "Epoch 1100, Loss: 944.6273027237729\n",
            "Epoch 1200, Loss: 587.191767549813\n",
            "Epoch 1300, Loss: 314.75646920236125\n",
            "Epoch 1400, Loss: 159.72469063845008\n",
            "Epoch 1500, Loss: 86.02187789859262\n",
            "Epoch 1600, Loss: 49.782296086617436\n",
            "Epoch 1700, Loss: 30.426244078726434\n",
            "Epoch 1800, Loss: 19.346134681014462\n",
            "Epoch 1900, Loss: 12.672391539221012\n",
            "Epoch 2000, Loss: 8.497187267500538\n",
            "Epoch 2100, Loss: 5.8075757611229255\n",
            "Epoch 2200, Loss: 4.034449449169161\n",
            "Epoch 2300, Loss: 2.843319088375094\n",
            "Epoch 2400, Loss: 2.0303096041270527\n",
            "Epoch 2500, Loss: 1.4675266722003533\n",
            "Epoch 2600, Loss: 1.072911369061397\n",
            "Epoch 2700, Loss: 0.7928613868683119\n",
            "Epoch 2800, Loss: 0.5918383801144536\n",
            "Epoch 2900, Loss: 0.44597191377874335\n",
            "Epoch 3000, Loss: 0.33903609489112657\n",
            "Epoch 3100, Loss: 0.2598758195657196\n",
            "Epoch 3200, Loss: 0.2007378886150874\n",
            "Epoch 3300, Loss: 0.15617617786763163\n",
            "Epoch 3400, Loss: 0.12232569835663623\n",
            "Epoch 3500, Loss: 0.0964163099901046\n",
            "Epoch 3600, Loss: 0.07644368790563832\n",
            "Epoch 3700, Loss: 0.0609444654496362\n",
            "Epoch 3800, Loss: 0.04884104920405948\n",
            "Epoch 3900, Loss: 0.0393334743963109\n",
            "Epoch 4000, Loss: 0.03182332426085949\n",
            "Epoch 4100, Loss: 0.025859713323368443\n",
            "Epoch 4200, Loss: 0.02110059627659486\n",
            "Epoch 4300, Loss: 0.01728482000927032\n",
            "Epoch 4400, Loss: 0.014211773703593111\n",
            "Epoch 4500, Loss: 0.011726458715568716\n",
            "Epoch 4600, Loss: 0.009708456081713267\n",
            "Epoch 4700, Loss: 0.008063718784878833\n",
            "Epoch 4800, Loss: 0.0067184262984444024\n",
            "Epoch 4900, Loss: 0.005614355217749664\n",
            "Epoch 5000, Loss: 0.00470537177466872\n",
            "Epoch 100, Loss: 17044.14906829523\n",
            "Epoch 200, Loss: 6852.315624044796\n",
            "Epoch 300, Loss: 6670.231491039413\n",
            "Epoch 400, Loss: 6313.427279186506\n",
            "Epoch 500, Loss: 5353.488352583484\n",
            "Epoch 600, Loss: 4082.3169464201846\n",
            "Epoch 700, Loss: 3276.1033923633336\n",
            "Epoch 800, Loss: 2655.921032614406\n",
            "Epoch 900, Loss: 2124.4296459601123\n",
            "Epoch 1000, Loss: 1694.5186028940661\n",
            "Epoch 1100, Loss: 1319.0987977466675\n",
            "Epoch 1200, Loss: 1000.093878356257\n",
            "Epoch 1300, Loss: 743.5395322040107\n",
            "Epoch 1400, Loss: 566.210134676562\n",
            "Epoch 1500, Loss: 437.0483746487922\n",
            "Epoch 1600, Loss: 327.5715738435151\n",
            "Epoch 1700, Loss: 248.10199583265577\n",
            "Epoch 1800, Loss: 202.372066822342\n",
            "Epoch 1900, Loss: 177.46202650387238\n",
            "Epoch 2000, Loss: 162.10968494375211\n",
            "Epoch 2100, Loss: 149.56655381134624\n",
            "Epoch 2200, Loss: 135.7257473718864\n",
            "Epoch 2300, Loss: 118.36132938932194\n",
            "Epoch 2400, Loss: 97.42596701739532\n",
            "Epoch 2500, Loss: 75.16125122528486\n",
            "Epoch 2600, Loss: 54.925968663921175\n",
            "Epoch 2700, Loss: 39.00974179379505\n",
            "Epoch 2800, Loss: 27.599147643149767\n",
            "Epoch 2900, Loss: 19.697355049655407\n",
            "Epoch 3000, Loss: 14.223004358106198\n",
            "Epoch 3100, Loss: 10.38224408523078\n",
            "Epoch 3200, Loss: 7.650155569948776\n",
            "Epoch 3300, Loss: 5.683356479988865\n",
            "Epoch 3400, Loss: 4.253267665686653\n",
            "Epoch 3500, Loss: 3.2044690507007783\n",
            "Epoch 3600, Loss: 2.4293742468173383\n",
            "Epoch 3700, Loss: 1.8524742557701703\n",
            "Epoch 3800, Loss: 1.4202079415074325\n",
            "Epoch 3900, Loss: 1.094254499682269\n",
            "Epoch 4000, Loss: 0.846990174889613\n",
            "Epoch 4100, Loss: 0.6583615433356381\n",
            "Epoch 4200, Loss: 0.5137101977118004\n",
            "Epoch 4300, Loss: 0.4022476231316866\n",
            "Epoch 4400, Loss: 0.31597933105245324\n",
            "Epoch 4500, Loss: 0.24894160703202425\n",
            "Epoch 4600, Loss: 0.19665689960096797\n",
            "Epoch 4700, Loss: 0.15574281788364636\n",
            "Epoch 4800, Loss: 0.12362956030074047\n",
            "Epoch 4900, Loss: 0.09835429123955386\n",
            "Epoch 5000, Loss: 0.07841045094308338\n",
            "Epoch 100, Loss: 25288.69486132683\n",
            "Epoch 200, Loss: 8462.795599233617\n",
            "Epoch 300, Loss: 7505.727492007738\n",
            "Epoch 400, Loss: 6428.523880055962\n",
            "Epoch 500, Loss: 4891.543547145405\n",
            "Epoch 600, Loss: 3273.8842541184126\n",
            "Epoch 700, Loss: 2036.9054578309376\n",
            "Epoch 800, Loss: 1228.6977295001993\n",
            "Epoch 900, Loss: 804.8201552709719\n",
            "Epoch 1000, Loss: 569.2566891074148\n",
            "Epoch 1100, Loss: 395.15015641087354\n",
            "Epoch 1200, Loss: 274.64841184689266\n",
            "Epoch 1300, Loss: 201.82071724835387\n",
            "Epoch 1400, Loss: 154.87009420800695\n",
            "Epoch 1500, Loss: 117.91905684784508\n",
            "Epoch 1600, Loss: 85.88233511785505\n",
            "Epoch 1700, Loss: 60.01529193089071\n",
            "Epoch 1800, Loss: 41.367074120754495\n",
            "Epoch 1900, Loss: 28.702290342426664\n",
            "Epoch 2000, Loss: 20.163111143665013\n",
            "Epoch 2100, Loss: 14.331144611199422\n",
            "Epoch 2200, Loss: 10.287083994620094\n",
            "Epoch 2300, Loss: 7.446108593547621\n",
            "Epoch 2400, Loss: 5.429082841460897\n",
            "Epoch 2500, Loss: 3.9845203314164848\n",
            "Epoch 2600, Loss: 2.942292235540105\n",
            "Epoch 2700, Loss: 2.1854656910026895\n",
            "Epoch 2800, Loss: 1.632648372223995\n",
            "Epoch 2900, Loss: 1.226613428708919\n",
            "Epoch 3000, Loss: 0.9267967129720014\n",
            "Epoch 3100, Loss: 0.7042510001943529\n",
            "Epoch 3200, Loss: 0.538199393956488\n",
            "Epoch 3300, Loss: 0.413652168654309\n",
            "Epoch 3400, Loss: 0.3197431049583226\n",
            "Epoch 3500, Loss: 0.2485592478881438\n",
            "Epoch 3600, Loss: 0.19431262674586763\n",
            "Epoch 3700, Loss: 0.15275092830286432\n",
            "Epoch 3800, Loss: 0.12073624792740116\n",
            "Epoch 3900, Loss: 0.09594271075101707\n",
            "Epoch 4000, Loss: 0.07663855468475528\n",
            "Epoch 4100, Loss: 0.061528477045063444\n",
            "Epoch 4200, Loss: 0.04963914497554937\n",
            "Epoch 4300, Loss: 0.04023573547072778\n",
            "Epoch 4400, Loss: 0.03276086194466527\n",
            "Epoch 4500, Loss: 0.026789709229941568\n",
            "Epoch 4600, Loss: 0.02199694581325626\n",
            "Epoch 4700, Loss: 0.018132224423378314\n",
            "Epoch 4800, Loss: 0.015001968424594114\n",
            "Epoch 4900, Loss: 0.012455775841746243\n",
            "Epoch 5000, Loss: 0.010376228309696296\n",
            "Epoch 100, Loss: 8434.410610594732\n",
            "Epoch 200, Loss: 7948.80203570619\n",
            "Epoch 300, Loss: 7468.757407653976\n",
            "Epoch 400, Loss: 6580.111569872577\n",
            "Epoch 500, Loss: 5620.2666667829735\n",
            "Epoch 600, Loss: 4368.072624511758\n",
            "Epoch 700, Loss: 3195.56823191787\n",
            "Epoch 800, Loss: 2286.4674389504617\n",
            "Epoch 900, Loss: 1603.7142691342958\n",
            "Epoch 1000, Loss: 1101.0612883353874\n",
            "Epoch 1100, Loss: 785.2143763589488\n",
            "Epoch 1200, Loss: 569.8797944526029\n",
            "Epoch 1300, Loss: 375.8940552473047\n",
            "Epoch 1400, Loss: 227.4637862440562\n",
            "Epoch 1500, Loss: 154.37072158476596\n",
            "Epoch 1600, Loss: 115.09945695962438\n",
            "Epoch 1700, Loss: 88.1729261858432\n",
            "Epoch 1800, Loss: 68.14620084157002\n",
            "Epoch 1900, Loss: 52.970383413775714\n",
            "Epoch 2000, Loss: 41.342909255837505\n",
            "Epoch 2100, Loss: 32.35491192779632\n",
            "Epoch 2200, Loss: 25.368963102192588\n",
            "Epoch 2300, Loss: 19.92532065205869\n",
            "Epoch 2400, Loss: 15.679682232628538\n",
            "Epoch 2500, Loss: 12.366973909691406\n",
            "Epoch 2600, Loss: 9.780452013845757\n",
            "Epoch 2700, Loss: 7.758438100164367\n",
            "Epoch 2800, Loss: 6.174783544062486\n",
            "Epoch 2900, Loss: 4.931424131306533\n",
            "Epoch 3000, Loss: 3.952383851293611\n",
            "Epoch 3100, Loss: 3.178939101263237\n",
            "Epoch 3200, Loss: 2.565755797836951\n",
            "Epoch 3300, Loss: 2.07783925897793\n",
            "Epoch 3400, Loss: 1.688150927993886\n",
            "Epoch 3500, Loss: 1.375761896139134\n",
            "Epoch 3600, Loss: 1.1244315249332646\n",
            "Epoch 3700, Loss: 0.9215182929950891\n",
            "Epoch 3800, Loss: 0.7571476147839851\n",
            "Epoch 3900, Loss: 0.6235768779849118\n",
            "Epoch 4000, Loss: 0.514710994858478\n",
            "Epoch 4100, Loss: 0.42573240242467386\n",
            "Epoch 4200, Loss: 0.3528179151237504\n",
            "Epoch 4300, Loss: 0.2929214519503858\n",
            "Epoch 4400, Loss: 0.24360676089955205\n",
            "Epoch 4500, Loss: 0.20291815445638625\n",
            "Epoch 4600, Loss: 0.16928021594864767\n",
            "Epoch 4700, Loss: 0.1414196562184758\n",
            "Epoch 4800, Loss: 0.11830416743286157\n",
            "Epoch 4900, Loss: 0.0990943717107203\n",
            "Epoch 5000, Loss: 0.08310590068645249\n",
            "Epoch 100, Loss: 29053.38813870479\n",
            "Epoch 200, Loss: 7702.289202177127\n",
            "Epoch 300, Loss: 7278.364329783972\n",
            "Epoch 400, Loss: 6354.93061665491\n",
            "Epoch 500, Loss: 5371.093722786188\n",
            "Epoch 600, Loss: 4148.416883071231\n",
            "Epoch 700, Loss: 2869.50701838022\n",
            "Epoch 800, Loss: 2026.9052157907709\n",
            "Epoch 900, Loss: 1415.413664530984\n",
            "Epoch 1000, Loss: 959.3774353571031\n",
            "Epoch 1100, Loss: 670.4066394536101\n",
            "Epoch 1200, Loss: 509.0708046276183\n",
            "Epoch 1300, Loss: 422.83972694981026\n",
            "Epoch 1400, Loss: 372.2387337634917\n",
            "Epoch 1500, Loss: 331.6477866989232\n",
            "Epoch 1600, Loss: 282.6390807468847\n",
            "Epoch 1700, Loss: 218.35169086450128\n",
            "Epoch 1800, Loss: 155.77174384683804\n",
            "Epoch 1900, Loss: 113.41983223382738\n",
            "Epoch 2000, Loss: 88.5804278561648\n",
            "Epoch 2100, Loss: 72.43542818373147\n",
            "Epoch 2200, Loss: 60.157939207090905\n",
            "Epoch 2300, Loss: 50.02169639952145\n",
            "Epoch 2400, Loss: 41.49553648707966\n",
            "Epoch 2500, Loss: 34.36106136200921\n",
            "Epoch 2600, Loss: 28.44245290502475\n",
            "Epoch 2700, Loss: 23.560900195108097\n",
            "Epoch 2800, Loss: 19.544211296039144\n",
            "Epoch 2900, Loss: 16.238992912915542\n",
            "Epoch 3000, Loss: 13.515629617247463\n",
            "Epoch 3100, Loss: 11.267724013706356\n",
            "Epoch 3200, Loss: 9.408995583325419\n",
            "Epoch 3300, Loss: 7.869618642583203\n",
            "Epoch 3400, Loss: 6.5929148458784885\n",
            "Epoch 3500, Loss: 5.532668595562893\n",
            "Epoch 3600, Loss: 4.651038997622134\n",
            "Epoch 3700, Loss: 3.9169491726211825\n",
            "Epoch 3800, Loss: 3.3048310419713127\n",
            "Epoch 3900, Loss: 2.793629938526301\n",
            "Epoch 4000, Loss: 2.366001722163344\n",
            "Epoch 4100, Loss: 2.0076569000761224\n",
            "Epoch 4200, Loss: 1.706820791175426\n",
            "Epoch 4300, Loss: 1.4537877802603651\n",
            "Epoch 4400, Loss: 1.2405531654791173\n",
            "Epoch 4500, Loss: 1.060509511171817\n",
            "Epoch 4600, Loss: 0.9081967202247127\n",
            "Epoch 4700, Loss: 0.7790967589831868\n",
            "Epoch 4800, Loss: 0.6694653650015807\n",
            "Epoch 4900, Loss: 0.576194261445685\n",
            "Epoch 5000, Loss: 0.4966984404421209\n",
            "Epoch 100, Loss: 15661.907396094684\n",
            "Epoch 200, Loss: 13315.727189517804\n",
            "Epoch 300, Loss: 5995.49819067629\n",
            "Epoch 400, Loss: 5801.832672955699\n",
            "Epoch 500, Loss: 5220.8998265808705\n",
            "Epoch 600, Loss: 4429.679860209833\n",
            "Epoch 700, Loss: 3661.747083565397\n",
            "Epoch 800, Loss: 2847.227761967409\n",
            "Epoch 900, Loss: 2072.1900430233304\n",
            "Epoch 1000, Loss: 1461.7856897046256\n",
            "Epoch 1100, Loss: 1040.8609012349214\n",
            "Epoch 1200, Loss: 744.9414696268988\n",
            "Epoch 1300, Loss: 532.3316861873876\n",
            "Epoch 1400, Loss: 372.05863777059034\n",
            "Epoch 1500, Loss: 254.83024163916954\n",
            "Epoch 1600, Loss: 173.36362098753384\n",
            "Epoch 1700, Loss: 118.55189643917912\n",
            "Epoch 1800, Loss: 82.58657189234418\n",
            "Epoch 1900, Loss: 58.65274996556079\n",
            "Epoch 2000, Loss: 42.0988147920433\n",
            "Epoch 2100, Loss: 30.30708545576293\n",
            "Epoch 2200, Loss: 21.829386162686045\n",
            "Epoch 2300, Loss: 15.76193862972303\n",
            "Epoch 2400, Loss: 11.455140568138383\n",
            "Epoch 2500, Loss: 8.411984021893794\n",
            "Epoch 2600, Loss: 6.25683620116182\n",
            "Epoch 2700, Loss: 4.717123426442065\n",
            "Epoch 2800, Loss: 3.602497243720171\n",
            "Epoch 2900, Loss: 2.783249039016033\n",
            "Epoch 3000, Loss: 2.171829296226247\n",
            "Epoch 3100, Loss: 1.7089913009895594\n",
            "Epoch 3200, Loss: 1.3542031581228302\n",
            "Epoch 3300, Loss: 1.0792979097371465\n",
            "Epoch 3400, Loss: 0.8643496924766569\n",
            "Epoch 3500, Loss: 0.6950054020815681\n",
            "Epoch 3600, Loss: 0.560746900812365\n",
            "Epoch 3700, Loss: 0.4537447288118023\n",
            "Epoch 3800, Loss: 0.36808952234638453\n",
            "Epoch 3900, Loss: 0.29926725789238334\n",
            "Epoch 4000, Loss: 0.24379422540045573\n",
            "Epoch 4100, Loss: 0.1989584172787656\n",
            "Epoch 4200, Loss: 0.16263309510006924\n",
            "Epoch 4300, Loss: 0.13314021023524192\n",
            "Epoch 4400, Loss: 0.10914888089043961\n",
            "Epoch 4500, Loss: 0.08959894679268118\n",
            "Epoch 4600, Loss: 0.0736427539569769\n",
            "Epoch 4700, Loss: 0.060600388667929286\n",
            "Epoch 4800, Loss: 0.04992496620604742\n",
            "Epoch 4900, Loss: 0.04117552538568105\n",
            "Epoch 5000, Loss: 0.03399573562509259\n",
            "Epoch 100, Loss: 6463.283990961308\n",
            "Epoch 200, Loss: 6090.663311797018\n",
            "Epoch 300, Loss: 5832.065360669003\n",
            "Epoch 400, Loss: 5371.322960224867\n",
            "Epoch 500, Loss: 4755.479209747789\n",
            "Epoch 600, Loss: 3995.8580873173523\n",
            "Epoch 700, Loss: 3006.079254053118\n",
            "Epoch 800, Loss: 2109.672392426934\n",
            "Epoch 900, Loss: 1510.1440478213017\n",
            "Epoch 1000, Loss: 1126.433495813103\n",
            "Epoch 1100, Loss: 851.7747524084748\n",
            "Epoch 1200, Loss: 629.8905602145829\n",
            "Epoch 1300, Loss: 453.5503307889171\n",
            "Epoch 1400, Loss: 336.65884943379746\n",
            "Epoch 1500, Loss: 267.0605324701091\n",
            "Epoch 1600, Loss: 219.7764731125155\n",
            "Epoch 1700, Loss: 180.6340497803503\n",
            "Epoch 1800, Loss: 144.98291820865083\n",
            "Epoch 1900, Loss: 113.66727872955306\n",
            "Epoch 2000, Loss: 88.31340889132828\n",
            "Epoch 2100, Loss: 68.75486495270694\n",
            "Epoch 2200, Loss: 53.78953893276782\n",
            "Epoch 2300, Loss: 42.23553703100185\n",
            "Epoch 2400, Loss: 33.214950896897946\n",
            "Epoch 2500, Loss: 26.11280284248342\n",
            "Epoch 2600, Loss: 20.494907749744883\n",
            "Epoch 2700, Loss: 16.046173341008277\n",
            "Epoch 2800, Loss: 12.529908523115266\n",
            "Epoch 2900, Loss: 9.761756729680604\n",
            "Epoch 3000, Loss: 7.593615611400822\n",
            "Epoch 3100, Loss: 5.904218200188276\n",
            "Epoch 3200, Loss: 4.593743449070953\n",
            "Epoch 3300, Loss: 3.580495769120969\n",
            "Epoch 3400, Loss: 2.7984285332729613\n",
            "Epoch 3500, Loss: 2.1949220626650265\n",
            "Epoch 3600, Loss: 1.7286439801599753\n",
            "Epoch 3700, Loss: 1.3675194959456385\n",
            "Epoch 3800, Loss: 1.0868910232468256\n",
            "Epoch 3900, Loss: 0.8679270744826643\n",
            "Epoch 4000, Loss: 0.6963018855706077\n",
            "Epoch 4100, Loss: 0.5611347753093041\n",
            "Epoch 4200, Loss: 0.4541589356724767\n",
            "Epoch 4300, Loss: 0.3690816981796044\n",
            "Epoch 4400, Loss: 0.30109837775442894\n",
            "Epoch 4500, Loss: 0.24652590573064187\n",
            "Epoch 4600, Loss: 0.20252809694753598\n",
            "Epoch 4700, Loss: 0.16691009149191272\n",
            "Epoch 4800, Loss: 0.13796457045818294\n",
            "Epoch 4900, Loss: 0.11435652771732184\n",
            "Epoch 5000, Loss: 0.09503668815902586\n",
            " Метод: pocket\n",
            " Среднее время выполнения: 0.054057 сек\n",
            " Средняя ошибка восстановления (MSE): 0.000098\n",
            " Метод: gradient\n",
            " Среднее время выполнения: 7.615970 сек\n",
            " Средняя ошибка восстановления (MSE): 0.084506\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "results = {'pocket': {'execution_time': [],\n",
        "                      'mse_values': []},\n",
        "           'gradient': {'execution_time': [],\n",
        "                        'mse_values': []}}\n",
        "\n",
        "sample_count = 10\n",
        "# взял поменьше, чтобы не жать тыщу лет\n",
        "tensor_shape = (30, 50, 80)\n",
        "tensor_rank = 10\n",
        "\n",
        "noisy_tensors = []\n",
        "for _ in range(sample_count):\n",
        "    tensor_data, _, _ = get_tensor(size=tensor_shape, r=tensor_rank)\n",
        "    # шумим\n",
        "    noisy_tensors.append(tensor_data + torch.randn_like(tensor_data) * 1e-2)\n",
        "\n",
        "# градиент\n",
        "for tensor in noisy_tensors:\n",
        "    start_time = time.time()\n",
        "    core_tensor, factors, _ = train(tensor, rank=[tensor_rank]*3, lr=1e-3, epochs=5000)\n",
        "    approx_tensor = rebuild_tensor(core_tensor, factors)\n",
        "\n",
        "    mse_value = torch.nn.functional.mse_loss(approx_tensor, tensor).item()\n",
        "    duration = time.time() - start_time\n",
        "\n",
        "    results['gradient']['execution_time'].append(duration)\n",
        "    results['gradient']['mse_values'].append(mse_value)\n",
        "\n",
        "# пакет\n",
        "for tensor in noisy_tensors:\n",
        "    start_time = time.time()\n",
        "    core_tensor, factors = tucker(tl.tensor(tensor), rank=[tensor_rank]*3)\n",
        "    core_tensor = torch.tensor(core_tensor, dtype=torch.double)\n",
        "    factors = [torch.tensor(factor, dtype=torch.double) for factor in factors]\n",
        "\n",
        "    reconstructed_tensor = rebuild_tensor(core_tensor, factors)\n",
        "\n",
        "    # MSE\n",
        "    mse_value = torch.nn.functional.mse_loss(reconstructed_tensor, tensor).item()\n",
        "    duration = time.time() - start_time\n",
        "\n",
        "    results['pocket']['execution_time'].append(duration)\n",
        "    results['pocket']['mse_values'].append(mse_value)\n",
        "\n",
        "\n",
        "for method in results:\n",
        "    average_time = sum(results[method]['execution_time']) / sample_count\n",
        "    print(f\" Метод: {method}\")\n",
        "    print(f\" Среднее время выполнения: {average_time:.6f} сек\")\n",
        "    average_mse = sum(results[method]['mse_values']) / sample_count\n",
        "    print(f\" Средняя ошибка восстановления (MSE): {average_mse:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3w2GLoOR-Ja"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYp3Lq4XR-LX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWAQuBSzUPpd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}